\documentclass[a4paper]{article}

\usepackage[margin=1cm]{geometry}

\title{Responsible AI and Machine Learning Explanation}
\author{Daniel Hess - 21971897}


\begin{document}
\maketitle

\section*{Introduction}
With the prospects of modern data analysis and machine learning, it is becoming clearer to everyone that data is becoming one of the most valuable assets in the world. With big data a present reality, increasingly powerful models are necessary to breakthrough in any competitive decision-making task. However, to decision makers with more critical choices, inherent trust in a model is more valuable than any metric you can present to sell them your solution. Often, we find that while the most powerful models can distinguish themselves in terms of normal performance metrics, can also struggle with generalization and can lead to a decrease with “in the wild” performance. These black-box models suffer from low interpretability, so if proper care is not taken in training and selection, the result is unclear reasons for performance drops with few options for rectification. Thus, deployment of these models in fully autonomous settings requires trust that the model will continue to perform within expectation; or in settings where a human will make the final decision: trust that the model is giving informed advice. This is a problem with inherently uninterpretable models, as they are often too complex to breakdown the inner workings of to understand and trust, or otherwise defeats the purpose of using them in the first place. \\

Real world data is not the same as validation data. Data leakage and other unknown factors can lead to unknown correlations appearing in complex models. The trust that a data scientist has in a model is inherently different to the trust that the end-user has: the end user should be able to question whether they trust a model, independently from the person who created it. Thus, models that are explainable and interpretable often have inherent benefits over picking the model with the best performance. \\

Ribeiro et al propose a solution to the explainability and interpretability of black-box machine learning models. By considering local neighborhoods around a test instance, and transformation onto a human-interpretable feature set: an explainable model can be fit to show easily interpretable features and possible local decision boundaries. \\

\vspace{0.5cm}

The system they present is called LIME (Local Interpretable Model-agnostic Explanations) which takes a single sample and a model, and then recreates a locally faithful explainable model. The first step is to define a human-interpretable feature space – this system does not require that the original model be fit on this feature space, but for interpretability requires that the feature be human-readable. E.g., for natural language processing (NLP), a model may be dependent on word-embeddings (which will be the original feature space X), and a human-readable representation may just be a vector representing the presence or absence of a particle word (which will be the representation space X’). Secondly, we take a single sample or case we want to explain, and a small neighborhood around its representation (both feature space and representation space values). With the original model, we can find labels for the neighborhood values. Finally with the labels for these samples, we can fit a secondary model with something interpretable (linear model, decision tree etc.), weighting samples that are closer to the original sample more heavily than the samples further away. With this you have an interpretable, and explainable model that is faithful for small regions around the original sample. \\

What this allows for is that given some instances that require some sense of explanation, we can find the features associated with a given results and their relative importance in the final output. An example the paper gives is they train a model to distinguish between pictures of wolves and huskies, however, deliberately train with data where wolves only appear in snow, and huskies without. Unsurprisingly, when the model is given an instance of a husky in the snow, the model predicts that it is a wolf. Looking at the explanation of important super-pixels, the model only shows the regions with snow – a critical point in validating the bias of this model. \\

In addition to this system, they present a method for selecting these representative instances to explain. While decision makers want to know they can trust a model, it is nonsensical and infeasible to expect them to check the explanation for all instances, thus selecting representatives is important. If we start by recording the explanations for all instances, we can calculate an importance value for each instance (with instances explained by more features being more important). Now, with the importances, we start by selecting the most important instance, then for each instance after, select the next instance that maximizes feature coverage. \\

This method has the benefit that you can continue to add instances to confirm or deny your trust in a model. Sometimes users may find certain features inherently untrustworthy so their presence or absence in the explanation may result in accepting or rejecting a model. 
\section*{Literature Review}
%Paper 3: Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI 

While the concept seems simple, the goal of explainability can be different for across tasks, users and mindsets. Arrieta et al \cite{arrieta2020explainable} have written a good overview of the concepts of explainability in artificial intelligence. They highlight many of the goals of explainability in AI and how it can lead to responsible machine learning and data science. Trustworthiness, accessibility, privacy, and confidence are just some of the main goals for explainability and they impact users differently. For example, an individual diagnosed with an illness from a machine learning model will be more concerned with the trustworthiness of that model than accessibility, thus different explanations are required by different individuals. For the purposes of this paper we will be mostly be looking at trustworthiness as it is considered to be the primary goal of explainable AI \cite{kim2015ibcm}\cite{ribeiro2016should}. Trustworthiness can be considered as a confidence in the ability for the model to perform as expected, however is on an individual basis. We would expect experts, users and people affected by a models decision to be the individuals concerned with a models trustworthiness.\\

%Paper 4: Working with Beliefs: AI Transparency in the Enterprise 

With increasing availability to different markets, machine learning has far surpassed it's adoption only to scholars. Chander et al \cite{chander2018working} discuss transparency in the business environment. They strongly suggest that for business to remain competitive in the many of the current markets, they must integrate AI into their workflows or lose to their competitors, thus making better AI solutions more attractive. There are key areas that a business can augment to maximize performance in the modern era, namely: human sensing and data acquisition with automatic sensing and logging, human decision-making, and finally human actionable items. However, before businesses can implement AI into these areas, they must first experience what AI can do for their business, and how they can trust and verify the model to perform as expected. In addition to this, there still exists a lot of misunderstanding and fear in how AI systems may change workplace behaviors and workflows. Transparency is what the team thinks will improve practical adoption and outline their “4-pillars of Transparency”: Accessibility, Explainability, Interactivity and Tunability. Interactivity and Tunability refers to the day-to-day usage of the end users and accounts for the fluidity of usage and ability to update models to better fit certain applications. Accessibility and Explainability means that an individual should be able to ask questions of the AI and the AI give some explanation for that user. They also identify key points about recommendations, identifying that models and systems that align with the biases of the end-user may fall under less scrutiny and continue to compound on those biases. However, models that go against preconceived beliefs may be viewed more critically. Thus, transparent and explainable systems are key for developing trust. \\

%Paper 2: An Efficient Explanation of Individual Classifications using Game Theory 

Often trust in a model’s ability to generalize is what allows it to be deployed and accepted. There are some areas, such as in the medical field, in which machine learning systems are hesitantly deployed. In a field that can mean life and death, inherent trust in a system is rare and often comes at other costs. The ability to see the explanation why a model has made a prediction can alleviate some of the potential downside of black-box systems.  \\

\v{S}trumbelj et al \cite{strumbelj2010efficient} consider a model with a given feature set, and an instance to inspect. Given a subset of features to inspect, we can determine the relative performance of that subset by determining the change in probability (or confidence) in our classification. Consider the titanic dataset: the prior probability of a passenger’s survival is 32\%, however, if that passenger is female, our confidence in their survival increases – thus we can note that increase in confidence for that subset of features. The original paper by \v{S}trumbelj et al \cite{vstrumbelj2009explaining} determines that to find relative contributions of all features, we need to look at the change in confidence for every subset of the feature space – that is to say, all elements of the powerset of the feature set. In the original paper, the team note that this takes exponential time, however in this paper make a few key improvements. Drawing inspiration from game theory, they notice that some of their equations can be rearranged such that it resembles Shapley’s value \cite{shapely1953value}. This brings about an iterative estimation method for the contribution values which converge rapidly, allowing for computation to be done in polynomial time. \\

How this method benefits over other methods of explanation (such as LIME \cite{ribeiro2016should}) is that it does not require refitting of the space, or estimation of the original model. What it does is explicitly calculate the effect and absolute contribution of a particular feature value on the estimation output. \\

%Paper 1: You Are the Only Possible Oracle: Effective Test Selection for End Users of Interactive Machine Learning Systems 

Often the only ones who can see the effects of properly tested models are end-users who need to effectively use these machine learning systems. Groce et al \cite{groce2013you} suggest that “you are the only possible oracle” relating to this sentiment. It is common for models to be built with only one end user, we see it all the time in computers, smartphones, and tailored advertising. However, in these cases, there is often no individual person or engineer testing these models for when they have faults – and they do have faults. In these cases, end users can sometimes see these faults in the form of mislabeled emails, irrelevant news articles, unimportant interruptions while working and countless others. While often these systems have some form of correction methods to help fit themselves better to a person’s needs, the most effective way of testing a system is to find its failure points. By better presenting the failure points and giving the model a chance to actively grow with the user, a system can interactively improve not only its performance, but also the trust the user has in the system to perform properly. Transparency in explaining a model’s shortcomings can do just this. \\

By looking at confidence, similarity, and distance measures, the team were able to find test suites that had significantly higher failure rates than the overall model. There are several ways they do this. Given a model and the test set, we can compute the relative confidence for each result in the set – thus if select results with low confidence then we may expect more failures. Another way they do this is by computing the average cosine distance from a test to all members of the training set, thus members with high cosine distance are more “unusual” and thus are more likely to fail. Finally, their “least-relevant” method prioritizes instances that are lacking the “most important” features. \\

Overall, this begs the question: “why do we care about finding failures?”. Users acquire and use new applications daily, and transparency with where they exceed and fail engages the user to envision its use cases. From the user’s side, this allows them to make a better decision about the output of certain models: where to trust and where not to. From the developer's side, this gives an exciting opportunity to engage with end users and, for those inclined, improve their models on a user-by-user basis. The article suggests that attempts to improve their models using these results may only be marginally effective, however user transparency and an increase in trust is an improvement.
\section*{Discussion}
In our team’s project, we are looking at Wideband Absorption data (WBA) and the prediction of conductive hearing loss in children. Our client has informed us that while we can obtain a large amount of WBA data very quickly from the person being tested, there currently exists little in the way of interpretation and that audiologists can find the data confusing and difficult to process. What this leads to is multiple older and slower methods whose effect (in conjunction with each other) results in a final diagnosis. What our client believes is that this data has the potential to replace all these techniques in a single diagnosis step. Thus, our goal is to goal is to perform analysis on this type of data and report how it influences a final prediction. In addition to this, depending on our results, we would also like to present our client with a model that they use to analyze this data quickly and efficiently. \\

Our main goal for our client is, more importantly, data explanation – we would like to present an explanation of how the data relates to the occurrence of hearing loss. The way we have approached this is using machine learning methods to fit the data to models, and by looking at alternative representations of the data. Because of the size of the data set, we have focused on simpler models (such as logistic regression and decision trees), primarily to avoid overfitting but also because they are inherently explainable and interpretable. In that sense we have followed the foundations of explainable machine learning has been relevant to our project. However, we have also considered more powerful black-box models, looking at what improvements can be made. Transparency and explainability are the two most important parts of our project, thus if choose to accept and present models such as random forests or neural networks, it is relevant that we review methods of explanation. That is where the advantage of a system like LIME is so beneficial, as if the input feature space is confusing we  can consider explanations in an arbitrary, human-readable feature space. \\

%Considerations 

From the beginning, there have always been things we have had to consider. The dataset size primarily has been our limiting factor in terms of scope and model choice. Powerful models require a large amount of data to be trained to the point to effectively generalize models, this means that if we are not careful, we risk overfitting in certain cases. What this also means is that model performance can be heavily influenced by training set selection and random state. We have had to be careful with our choices of what information to reject and our thought process in doing so. For myself, I have used the same training and test sets between models and feature sets. While this has the effect that some models will be crippled, and others will be boosted I can be sure that I have not let this influence choice. In addition to separating a test set from the training data, hyperparameter searching and model selection is all selected using LOOCV on the training data – minimizing the reference to test performance to avoid test bias. I am acutely aware of the existence of test bias falling for this trap a few times during this project. \\

Since we are in the medical space, assuming our model(s) get deployed, there is a real effect this model can have on people’s lives, unfortunately, not necessarily just for the better. Things that as a team we have discussed amongst ourselves and with our client is: what happens when something goes wrong? For us, luckily (in the case of binary classification), we have 2 types of failures: Type I and Type II. Type I errors account for false positives where our model has (mistakenly) identified a child as having an ear problem. What this looks like is a child being taken in for surgery, then under anesthetic, the surgeon looking into the child's ear and... there is no issue. While this may just seem like an inconvenience, there is an inherent risk of unnecessary surgery that could have been avoided, and of course the material component, money. Type II error, or false negatives, are what go unnoticed longer. Hearing problems can result in learning problems at school which can lead to lower paying jobs or lost time effectiveness or mental health issues, it is unknown what it can cause or cascade into. \\

We have implemented this thought process into how we evaluate models, not only using normal metrics such as sensitivity or specificity, but also relative societal cost. The cost of surgery (whether for someone with a problem or someone without) is something easily calculated but the other case is a bit harder to quantify. WHO did some calculations on what undiagnosed hearing can cost society in monetary terms. These calculations allow us to be better informed while looking at model performance, and additionally give perspective to values. While this does not really account for "cost" on the individual level, these things are beyond our scope of expertise.  \\

Overall, what I believe is the best course of action we can take is transparency. With our models we can stick with interpretable models, but we can use more powerful models and look at explainability methods. We need to be transparent with our performance and model expectations, so the client understands what they can expect from it typically. And finally, we need to be transparent with our methods: humans are fallible, and peer-review is a powerful tool that leads to trust in methods and tools.

\pagebreak
\nocite{*}
\bibliographystyle{abbrv}
\bibliography{refs}


\end{document}
